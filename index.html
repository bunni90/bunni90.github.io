<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Niklas Bunzel</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
</head>
<body>

    <header>
        <div class="menu-bar">
            <h1 class="logo">Niklas Bunzel</h1>
            <nav>
                <ul>
                    <li><a href="#home">Home</a></li>
                    <li><a href="#research">Research</a></li>
                    <li><a href="#talks">Talks</a></li>
                    <li><a href="#services">Services</a></li>
                    <li><a href="#cv">CV</a></li>
                    <li><a href="#publications">Publications</a></li>
                </ul>
            </nav>
        </div>   
    </header>
    <main>
    
        <section id="home" class="section">
        	<div class="home-container">
			<!-- Left: Text Content -->
			<div class="left-content">
			    <h2>Niklas Bunzel</h2>
			    <p>I am a research scientist at the Fraunhofer SIT and currently pursuing my PhD at the TU-Darmstadt. I specialize in artificial intelligence and IT security, focusing on adversarial machine learning and robustness in AI systems. My expertise includes developing defenses against adversarial attacks, such as evasion attacks, and advancing deepfake detection methods, supported by a portfolio of over 20 publications. As a core member of the OWASP AI Exchange I bridge technical safeguards with regulatory requirements. I regularly present at academic and industry conferences, sharing insights on adversarial ML and AI security. I am skilled in frameworks and programming languages such as PyTorch, Keras, and Python, and I create proofs of concept that bring my research to life. My work is driven by the goal of advancing secure, trustworthy, and impactful AI systems.</p>
			</div>
			
			<!-- Right: Image, Occupation, and Contact -->
			<div class="right-content">
			    <div class="image-container">
				<img src="Bunsel-14-1.jpg" alt="Profile Picture">
			    </div>
			    <div class="details">
				<p><strong>Research Scientist</strong></p>
				<div class="social-links">
				    <a href="https://scholar.google.com/citations?user=HoksiuwAAAAJ" target="_blank" title="Google Scholar">
					<i class="fas fa-graduation-cap"></i>
				    </a>
				    <a href="https://www.linkedin.com/in/niklas-bunzel-910a58199/" target="_blank" title="LinkedIn">
					<i class="fab fa-linkedin"></i>
				    </a>
				    <a href="mailto:firstname.lastname@sit.fraunhofer.de" title="Email">
					<i class="fas fa-envelope"></i>
				    </a>
				    <a href="https://github.com/bunni90" target="_blank" title="GitHub">
					<i class="fab fa-github"></i>
				    </a>
				    <a href="https://orcid.org/0000-0002-8921-1562" target="_blank" title="ORCID">
        				<i class="fab fa-orcid"></i>
    				    </a>
				</div>
			    </div>
			</div>
		
		</div>
            
        </section>
        <section id="research" class="section">
            <h2>Research</h2>
            <h3>AI Security/Trustworthy AI</h3>
            <p>The <a href="https://senpai.athene-center.de/projekte#c7153" target="_blank">RoMa</a> project is a research project that aims to improve the robustness of image classifiers against both benign environmental influences and adversarial attacks. The team is researching methods to defend against adversarial examples that make minimal changes to the image inputs, as well as against adversarial patches. The project is also working to raise public awareness of the potential security risks posed by adversarial attacks through publications, lectures, educational events and demonstrations.<br>
            
            We investigate adversarial attack detectors, with a particular emphasis on adversarial patch detection in both digital and physical domains, focusing on tasks such as face recognition, object detection, and deepfake detection. Our research also explores the transferability of evasion attacks, as well as the use of evasion attacks and adversarial training in continual learning environments. In addition, we advance AI safety by identifying and generating rare edge-case data and simulating adverse weather conditions to improve system robustness and reliability.</p>


            <h3>Deepfakes</h3>
            <p><a href="https://www.sit.fraunhofer.de/de/secmedid/" target="_blank">SecMedID</a>  focuses on addressing the escalating risks posed by deepfake technologies, which enable the highly realistic manipulation of faces and voices in digital media. These technologies threaten individuals, organizations, and society by facilitating fraud, extortion, and disinformation, eroding trust in media, compromising democratic processes, enabling defamation, tampering with legal evidence, and jeopardizing public safety through falsified communications. 

To counter these threats, the project investigates the state-of-the-art in deepfake methods for video and audio manipulation and advances techniques such as Face Swapping, Facial Reenactment, Voice Conversion, and Text-to-Speech to assess future risks. Additionally, it explores forensic detection mechanisms to identify deepfakes, even when adversarial attacks are employed to obscure their authenticity. By advancing knowledge and tools in both deepfake generation and detection, SecMedID seeks to ensure the integrity and trustworthiness of digital media.</p>
            <h3>Media Security & Steganography</h3>
            <p>We conduct research in the fields of media security, steganography, and steganalysis. Media security involves ensuring the integrity and authenticity of digital media, and our work includes developing robust image hashing techniques, particularly for law enforcement applications. Steganography involves concealing information within another medium, such as an image or video, while steganalysis focuses on detecting such hidden data. We have explored its forensic applications. For instance, we investigated scenarios where law enforcement agencies possess auxiliary knowledge, as well as the potential use of platforms like Telegram as a steganographic channel.</p>
        </section>
        
        <section id="talks" class="section">
            <h2>Talks</h2>
            <p>Details of my past and upcoming talks.</p>
            <ul class="centered-list">
            	<li> <b>GenAI in the Battle of Security: Attacks, Defenses, and the Laws Shaping AI's Future</b>, with <a href="https://www.linkedin.com/in/raphael-antonius-frick-937113314/" target="_blank">Raphael Antonius Frick</a>, German OWASP Day, 2024. Slides <a href="https://media.ccc.de/v/god2024-56275-genai-in-the-battle-of-sec#t=1388" target="_blank">Video</a></li>
		<li> <b>Creating a culture of security</b>, Keynote with <a href="https://www.linkedin.com/in/hectorib/" target="_blank">Hector Ibarra</a>, Amazon AWS re:Inforce re:Cap, 2024</li>
		<li> <b>Seeing is Not Always Believing: The Rise, Detection, and Evasion of Deepfakes</b>, with <a href="https://www.linkedin.com/in/raphael-antonius-frick-937113314/" target="_blank">Raphael Antonius Frick</a>, OWASP Frankfurt, 2023. <a href="https://owasp.org/www-chapter-frankfurt/assets/slides/63_OWASP_Frankfurt_Stammtisch_2.pdf" target="_blank">Slides</a></li>
		<li> <b>KI und Sicherheit​ - Maschinelles Lernen als Sicherheitsrisiko?</b>, with <a href="https://www.linkedin.com/in/verena-battis-b13709149/" target="_blank">Verena Battis</a>, Fachtagung Cyber Security, Banken Verlag, 2022</li>
            </ul>
        </section>
        
        <section id="services" class="section">
            <h2>Services</h2>
            <h3>Technical Program Committee Member</h3>
            <ul class="centered-list">
            	<li>2024: 21st Annual International Conference on Privacy, Security, and Trust</li>
		<li>2024: 25th International Conference on Web Information Systems Engineering</li>
		<li>2022: 6th International Workshop on Criminal Use of Information Hiding (CUING 2022)</li>
            </ul>
            <h3>Journal Reviewer</h3>
            <ul class="centered-list">
            	<li>IEEE Transactions on artificial intelligence (TAI)</li>
		<li>Computers & Security</li>
            </ul>
        </section>
        
        <section id="cv" class="section">
	    <h2>Curriculum Vitae</h2>
	    <div class="cv-container">
		<div class="cv-section">
		    <h3>Experience</h3>
		    <ul class="centered-list">
		        <li><strong>Core Member, OWASP AI Exchange</strong> (Since 11.2023)</li>
		        <li><strong>Research Scientist, Fraunhofer SIT</strong> (Since 04.2020)</li>
		        <li><strong>Software Engineer, Independent</strong> (12.2019–04.2020)</li>
		        <li><strong>Software Engineering \& Project Design, SÖRF GbR</strong> (03.2018--12.2019)</li>
		        <li><strong>Software Engineer, Independent</strong> (06.2016--02.2018)</li>
		        <li><strong>Student Assistant, TU Darmstadt</strong> (04.2013--09.2013)</li>
		    </ul>
		</div>
		<div class="cv-section">
		    <h3>Education</h3>
		    <ul class="centered-list">
		        <li><strong>PhD in Computer Science</strong>, TU-Darmstadt (2020–2025)</li>
		        <li><strong>Master of Science IT-Security</strong>, TU-Darmstadt (2015–2019)</li>
		        <li><strong>Master of Science Computer Science</strong>, TU-Darmstadt (2015–2019)</li>
		        <li><strong>Bachelor of Science Computer Science</strong>, TU-Darmstadt (2010–2015)</li>
		        <li><strong>Abitur</strong>, Martin-Niemöller Schule (2010–2015)</li>
		    </ul>
		</div>
		<div class="cv-section">
		    <h3>Skills</h3>
		    <ul class="centered-list">
		        <li><strong>Industry Knowledge:</strong> Machine Learning, Adversarial ML, Digital Signatures, IT-Forensics, Cryptography, PKI</li>
		        <li><strong>Programming Languages:</strong> Python, C#/.Net, PHP</li>
		        <li><strong>Frameworks:</strong> PyTorch, TensorFlow, Adversarial Robustness Toolbox</li>
		        <li><strong>Languages:</strong> German (native), English (fluent)</li>
		    </ul>
		</div>
		<div class="cv-section">
		    <h3>Certificates</h3>
		        <strong>Data Scientist Specialized in Trustworthy AI</strong>
		</div>
		<div class="cv-section">
		    <h3>Interests</h3>
		    <p>Volunteer fire brigade (Leading firefighter), Ving Chun (martial art), Board games.</p>
		</div>
	    </div>
	</section>
	
	<section id="publications" class="section">
	    <h2>Publications</h2>
	    <ul class="centered-list">
		<li><em>Analyzing the Effectiveness of Image Preprocessing Defenses Under Runtime Constraints.</em> TrustCom 2024</li>
		<li><em>Bridging the Gap: The Role of OWASP AI Exchange in AI Standardization.</em> <a href="https://dl.gi.de/items/9f8612f8-43a2-40ff-9be1-21d5b6b22235" target="_blank">INFORMATIK 2024</a></li>
		<li><em>Adversarial Patch Detection: Leveraging Depth Contrast for Enhanced Threat Visibility.</em> <a href="https://ieeexplore.ieee.org/document/10646903" target="_blank">DSML@DSN 2024</a></li>
		<li><em>Measuring the effects of environmental influences on Object Detection.</em> <a href="https://ieeexplore.ieee.org/document/10646929" target="_blank">DSML@DSN 2024</a></li>
		<li><em>Patching the Cracks: Detecting and Addressing Adversarial Examples in Real-World Applications.</em> <a href="https://ieeexplore.ieee.org/document/10647102" target="_blank">DSN 2024</a></li>
		<li><em>Signals Are All You Need: Detecting Digital and Real-World Adversarial Patches Using Signal-Based Features.</em> <a href="https://dl.acm.org/doi/abs/10.1145/3665451.3665530" target="_blank">SecTL@AsiaCCS 2024</a></li>
		<li><em>Identifying and Generating Edge Cases.</em> <a href="https://dl.acm.org/doi/10.1145/3665451.3665529" target="_blank">SecTL@AsiaCCS 2024</a></li>
		<li><em>Prediction of Flipped Bits in Robust Image Hashes by Machine Learning.</em> <a href="" target="_blank">EI 2024</a></li>
		<li><em>Transferrability of Adversarial Attacks from Convolutional Neural Networks to ChatGPT4.</em> <a href="https://publica.fraunhofer.de/entities/publication/d506b2ba-78fe-431d-83f8-72a82986fa1d" target="_blank">Publica 2023</a></li>
		<li><em>A Concise Analysis of Pasting Attacks and their Impact on Image Classification.</em> <a href="https://ieeexplore.ieee.org/document/10207094" target="_blank">DSML@DSN 2023</a></li>
		<li><em>Adversarial Patch Detection and Mitigation by Detecting High Entropy Regions.</em> <a href="https://ieeexplore.ieee.org/document/10207151" target="_blank">DSML@DSN 2023</a></li>
		<li><em>Multi-class Detection for Off The Shelf transfer-based Black Box Attacks.</em> <a href="https://dl.acm.org/doi/10.1145/3591197.3591305" target="_blank">SecTL@AsiaCCS 2023</a></li>
		<li><em>Detection of deepfakes using background-matching.</em> <a href="https://library.imaging.org/ei/articles/35/4/MWSF-381" target="_blank">EI 2023</a></li>
		<li><em>Predicting positions of flipped bits in robust image hashes.</em> <a href="https://library.imaging.org/ei/articles/36/4/MWSF-339" target="_blank">EI 2023</a></li>
		<li><em>Face Pasting Attack.</em> <a href="https://arxiv.org/abs/2210.09153" target="_blank">ARXIV 2022</a></li>
		<li><em>Using Telegram as a carrier for image steganography: Analysing Telegrams API limits.</em> <a href="https://dl.acm.org/doi/10.1145/3538969.3544440" target="_blank">CUING@ARES 2022</a></li>
		<li><em>Robust face recognition: How much face Is needed?.</em> <a href="https://library.imaging.org/ei/articles/34/4/MWSF-209" target="_blank">EI 2022</a></li>
		<li><em>Adversarial Examples zum Selbstdatenschutz? Der Fall biometrischer Gesichtserkennung im öffentlichen Raum.</em> <a href="https://dl.gi.de/items/1ae3a1bf-615f-49c8-b6d5-9079193f9e84" target="_blank">INFORMATIK 2021</a></li>
		<li><em>Cover-aware Steganalysis.</em> <a href="https://journals.riverpublishers.com/index.php/JCSANDM/article/view/5967" target="_blank">J. Cyber Secur. Mobil. 2021</a></li>
		<li><em>Non-Blind Steganalysis.</em> <a href="https://dl.acm.org/doi/10.1145/3407023.3409221" target="_blank">CUING@ARES 2020</a></li>
	    </ul>
	</section>

    </main>
    <footer>
        <p>&copy; 2024 Niklas Bunzel. All Rights Reserved.</p>        
    </footer>
    
    <script>
    let lastScrollPosition = 0;
    const menuBar = document.querySelector('.menu-bar');

    window.addEventListener('scroll', () => {
        const currentScrollPosition = window.pageYOffset || document.documentElement.scrollTop;

        if (currentScrollPosition > lastScrollPosition) {
            // Scrolling down - hide menu bar
            menuBar.style.transform = 'translateY(-100%)';
        } else {
            // Scrolling up - show menu bar
            menuBar.style.transform = 'translateY(0)';
        }

        lastScrollPosition = currentScrollPosition <= 0 ? 0 : currentScrollPosition; // Avoid negative scroll
    });
</script>
</body>
</html>

